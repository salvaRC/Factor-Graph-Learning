{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from examples.factors import accuracies, propensities\n",
    "from examples.prediction_and_evaluation import pred_and_eval_gen_model, eval_majority_vote\n",
    "from examples.utils import change_labels\n",
    "from factor_graph import FactorGraph\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing the implemented factor graph against Snorkel (a latent MRF model)\n",
    "\n",
    "## The data used consists of:\n",
    " - labels Y for the created task of discriminating professors from teachers in the Bias in Bios dataset\n",
    " - 99 selected labeling functions, usable for a standard data programming pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def train_supervised(label_matrix, Y_true, lf_prop=True, n_epoch=25, lr=0.1, gibbs_samples=10):\n",
    "    start_t = time.time()\n",
    "    n_LFs = label_matrix.shape[1]\n",
    "    \"\"\" Get polarities of each LF, ASSUMPTION: Each LF only votes for ONE label, and abstains otherwise\"\"\"\n",
    "    polarities = np.sign(np.sum(label_matrix, axis=0))\n",
    "    \"\"\" In the supervised case, the data fed into the PGM Learning will just be all concatenated \"\"\"\n",
    "    observations = np.concatenate((Y_true.reshape((-1, 1)), label_matrix), axis=1)\n",
    "    \"\"\" Create a MRF with fully observed variables\"\"\"\n",
    "    potentials = [(accuracies, n_LFs)]\n",
    "    if lf_prop:\n",
    "        potentials += [(propensities, n_LFs)]\n",
    "    lm = FactorGraph(n_vars=n_LFs+1, polarities=[-1] + list(polarities), potentials=potentials)\n",
    "    lm.fit(observations, lr=lr, n_epochs=n_epoch, batch_size=250, gibbs_samples=gibbs_samples, verbose=False)\n",
    "    \"\"\" Evaluate the learned generative model \"\"\"\n",
    "    stat, probs = pred_and_eval_gen_model(lm, observations, Y_true, version=99, abst=0, verbose=True, print_MV=False,\n",
    "                                          eps=0.0, return_preds=True, coverage_stats=False, add_prefix=\"\")\n",
    "    duration = time.time() - start_t\n",
    "    print(f\"Time needed by generative model: {duration}\")\n",
    "    # Will train the downstream classifier:\n",
    "    # stat_cl = train_and_eval_classifier(Xtrain, Xtest, probs, Ytest, label_matrix, library='torch',\n",
    "    #                                    optim='Adam', devicestring=device, epochs=250, print_step=505)\n",
    "    return lm, stat, probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train_snorkel(label_matrix, Y_true, n_epoch=1000, lr=0.1):\n",
    "    from snorkel.labeling.model import LabelModel\n",
    "    # LABEL MODEL\n",
    "    start_t = time.time()\n",
    "    \"\"\" Snorkel requires abstention label to be -1...\"\"\"\n",
    "    label_matrix, Y_true = change_labels(label_matrix, Y_true, new_label=-1, old_label=0)\n",
    "    \"\"\" Train latent label model from Snorkel \"\"\"\n",
    "    lm = LabelModel(cardinality=2)\n",
    "    lm.fit(label_matrix, n_epochs=n_epoch, seed=77, lr=lr)\n",
    "    \"\"\" Evaluate the learned generative model \"\"\"\n",
    "    stat, probs = pred_and_eval_gen_model(lm, label_matrix, Y_true, abst=-1, verbose=True,\n",
    "                                          print_MV=False, eps=0.0, MV_policy=\"random\",\n",
    "                                          return_preds=True, version=10, coverage_stats=False)\n",
    "\n",
    "    duration = time.time() - start_t\n",
    "    print(f\"Time needed by Snorkel's generative model: {duration}\")\n",
    "    # Will train the downstream classifier:\n",
    "    # stat_cl = train_and_eval_classifier(Xtrain, Xtest, probs, Ytest, label_matrix, library='torch',\n",
    "    #                                    optim='Adam', devicestring=device, epochs=250, print_step=505)\n",
    "    return lm, stat, probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "seed = 77\n",
    "n_runs = 5\n",
    "data = np.load(\"../data/professor_vs_teacher_99LFs.npz\")\n",
    "L_arr, Ytrain = data[\"L\"], data[\"Y\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------- MAJORITY VOTE STATS --------------------------------------------------\n",
      "MV on all samples with  99 LFs\n",
      "Majority vote stats:\n",
      "Accuracy:0.753 | Precision:0.770 | Recall:0.717 | F1 score:0.742 | AUC:0.796 | Log loss:5.506 | Brier:0.917 | Coverage:1.000 | MSE, MAE:0.917, 0.751\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------- MAJORITY VOTE STATS --------------------------------------------------\")\n",
    "print(\"MV on all samples with \", L_arr.shape[1], \"LFs\")\n",
    "eval_majority_vote(L_arr, Ytrain, abst=0, MV_policy='random')\n",
    "print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "# PRINT LF descriptions: [print(d) for d in descr]\n",
    "lfprop = False\n",
    "n_samples, nlf = L_arr.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised (ours)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.753 | Precision:0.694 | Recall:0.896 | F1 score:0.782 | AUC:0.888 | Log loss:1.586 | Brier:1.074 | Coverage:1.000 | MSE, MAE:1.074, 0.755\n",
      "Time needed by generative model: 5.789618015289307\n",
      "Accuracy:0.750 | Precision:0.693 | Recall:0.892 | F1 score:0.780 | AUC:0.881 | Log loss:2.865 | Brier:1.069 | Coverage:1.000 | MSE, MAE:1.069, 0.750\n",
      "Time needed by generative model: 13.907033681869507\n",
      "Accuracy:0.753 | Precision:0.696 | Recall:0.893 | F1 score:0.782 | AUC:0.881 | Log loss:2.861 | Brier:1.069 | Coverage:1.000 | MSE, MAE:1.069, 0.750\n",
      "Time needed by generative model: 13.33698320388794\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.1, n_epoch=10)\n",
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.1, n_epoch=25)\n",
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.1, n_epoch=25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.759 | Precision:0.701 | Recall:0.896 | F1 score:0.787 | AUC:0.891 | Log loss:1.451 | Brier:1.057 | Coverage:1.000 | MSE, MAE:1.057, 0.750\n",
      "Time needed by generative model: 3.4220330715179443\n",
      "Accuracy:0.758 | Precision:0.700 | Recall:0.895 | F1 score:0.785 | AUC:0.886 | Log loss:2.568 | Brier:1.051 | Coverage:1.000 | MSE, MAE:1.051, 0.744\n",
      "Time needed by generative model: 8.094967126846313\n",
      "Accuracy:0.762 | Precision:0.704 | Recall:0.897 | F1 score:0.789 | AUC:0.891 | Log loss:0.843 | Brier:1.062 | Coverage:1.000 | MSE, MAE:1.062, 0.758\n",
      "Time needed by generative model: 16.368001461029053\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.1, n_epoch=10, gibbs_samples=5)\n",
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.1, n_epoch=25, gibbs_samples=5)\n",
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.01, n_epoch=50, gibbs_samples=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.788 | Precision:0.758 | Recall:0.841 | F1 score:0.798 | AUC:0.889 | Log loss:0.657 | Brier:0.941 | Coverage:1.000 | MSE, MAE:0.941, 0.742\n",
      "Time needed by generative model: 8.22499942779541\n",
      "Accuracy:0.759 | Precision:0.701 | Recall:0.894 | F1 score:0.786 | AUC:0.892 | Log loss:0.617 | Brier:1.075 | Coverage:1.000 | MSE, MAE:1.075, 0.771\n",
      "Time needed by generative model: 31.87503147125244\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.01, n_epoch=50, gibbs_samples=1)\n",
    "_, _, _ = train_supervised(L_arr, Ytrain, lf_prop=lfprop, lr=0.003, n_epoch=100, gibbs_samples=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Snorkel\n",
    "### Note that this is the newer, faster snorkel. (the old snorkel using SGD+MLE is similarly slow or slower)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.769 | Precision:0.718 | Recall:0.878 | F1 score:0.790 | AUC:0.880 | Log loss:0.678 | Brier:0.159 | Coverage:1.000 | MSE, MAE:0.159, 0.275\n",
      "Time needed by Snorkel's generative model: 2.810915946960449\n",
      "Accuracy:0.769 | Precision:0.718 | Recall:0.878 | F1 score:0.790 | AUC:0.880 | Log loss:0.678 | Brier:0.159 | Coverage:1.000 | MSE, MAE:0.159, 0.275\n",
      "Time needed by Snorkel's generative model: 1.949998140335083\n",
      "Accuracy:0.769 | Precision:0.718 | Recall:0.878 | F1 score:0.790 | AUC:0.880 | Log loss:0.678 | Brier:0.159 | Coverage:1.000 | MSE, MAE:0.159, 0.275\n",
      "Time needed by Snorkel's generative model: 1.9000012874603271\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = train_snorkel(L_arr, Ytrain, lr=0.01, n_epoch=1000)\n",
    "_, _, _ = train_snorkel(L_arr, Ytrain, lr=0.01, n_epoch=1000)\n",
    "_, _, _ = train_snorkel(L_arr, Ytrain, lr=0.1, n_epoch=1000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}